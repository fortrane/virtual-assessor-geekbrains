# Установка Post-Backend части (Model)

Saiga LLaMa 3
Минимальные требования от сервера для модели:
- 24 GB VRAM
- 38 GB RAM
- SSD/HDD 20 GB (свободное место)

Saiga-Mistral
Минимальные требования от сервера для модели:
- 20 GB VRAM
- 24GB RAM 
- SSD/HDD 20 GB (свободное место)

OCR model
- 4 GB RAM


**Версия Python на тестах 3.9**

### Процесс установки для Windows RDP:
1. Скачайте контент папки llm_back
2. Переместите контент папки llm_back в удобное для вас место и инициализируйте venv
3. Установите зависимости через команду:
```python
  pip install -r requirements.txt
```

Архитектура: 
```text

llm_back│               
│
├── document_loaders.py             - Файл модуля Python, отвечающий за обработку документов, например, загрузка, парсинг документов DOCX.
│
├── llm_part.py                     - Файл модуля Python, где описана логика работы с Large Language Models (LLM).
│
├── main.py                         - Основной файл FastAPI, который запускает сервер и содержит определение маршрутов API, обработку запросов и возвращение ответов.
│
├── ocr.py                          - Файл модуля Python, предназначенный для работы с easyOCR, который может обрабатывать изображения и извлекать из них текст.
│
├── prompts.py                      - Файл модуля Python, где хранятся различные шаблоны промптов, которые использованы при взаимодействии с LLM.
│
└── req.py                          - Файл модуля Python, в котором содержатся функции для выполнения HTTP-запросов средствами библиотеки requests.


```
### Редактирование конфига
В req.py заменить `url_s = "http://127.0.0.1/src/Api/v1.php"` на ссылку к вашему Api backend 

### Для запуска требуется прописать в консоли:
```python
uvicorn main:app --reload --host server --port 8080
```
###
4. Переходим на http://127.0.0.1:8080/docs Тут у нас лежат эндпоинты к основным функциям.



